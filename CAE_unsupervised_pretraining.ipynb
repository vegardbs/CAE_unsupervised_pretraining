{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "supervised_and_unsupervised_class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odzhc_s4b_4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, Flatten, InputLayer, Conv2D, Conv2DTranspose, MaxPool2D\n",
        "from keras.layers import Dropout, GaussianNoise\n",
        "from keras import Sequential, Model\n",
        "from sklearn.utils import compute_class_weight\n",
        "import os\n",
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tied_convolution\n",
        "from tied_convolution import Conv2DTranspose_tied\n",
        "\n",
        "class CAE_classifier:\n",
        "    \"\"\" \n",
        "    Class to build and train combined autoencoder and classifier networks.\n",
        "\n",
        "    --MAIN METHODS--\n",
        "\n",
        "    build_autoencoder():\n",
        "      Method to build the encoder and autoencoder networks.\n",
        "      The encoder is passed as a layer to the autoencoder and classifier networks,\n",
        "      so to access the layers within the encoder block, use autoencoder.layers[0].layers\n",
        "      > Gets called when an instance of the class is created.\n",
        "\n",
        "    train_autoencoder(x_train,x_test):\n",
        "      Method to train the autoencoder. The input data and target data is the same,\n",
        "      since the goal is to reconstruct the input. Data shape has to be (-1,) + input_shape\n",
        "\n",
        "    build_classifier(pretrained = True):\n",
        "      Method to build a classifer model. \n",
        "      If the pretrained keyword is True, weights from the trained autoencoder gets\n",
        "      copied to the encoder block of the classifier. If it is False, the network\n",
        "      is initialized with the same architecture, but the weights and biases are random.\n",
        "      > Gets called by the train_classifer method.\n",
        "\n",
        "    train_classfier(data,val_data, pretrained = True):\n",
        "      Method to train a classifer model.\n",
        "      If the pretrained keyword is True, a model with pretrained weights from\n",
        "      the autoencoder is created. If it is False, an identical model with randomized\n",
        "      initial weights is crated.\n",
        "\n",
        "    evaluate_classifier(x_test, y_test, key = \"pretrained_classifier\"):\n",
        "      Method to evaluate the classfiers on test data.\n",
        "      Keyword determine which classifier should be evaluated.\n",
        "\n",
        "    --VISUALIZATION METHODS--\n",
        "\n",
        "    plot_reconstruction(image):\n",
        "      Method to plot the autoencoder reconstruction of provided image\n",
        "\n",
        "    plot_filters(key = \"autoencoder):\n",
        "      Plots the filters of the first convolutional layer.\n",
        "      Keyword determine which model the filters come from.\n",
        "\n",
        "    plot_feature_map(key = \"autoencoder):\n",
        "      Method to plot the feature map of the first convolutional layer.\n",
        "      Keyword determines which model the feature map come from.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.pretrained_classifier = None\n",
        "        self.rand_init_classifier = None\n",
        "        \n",
        "        self.autoencoder, self.encoder = self.build_autoencoder()\n",
        "\n",
        "    def build_autoencoder(self):\n",
        "\n",
        "        padding = \"same\"\n",
        "        optimizer = \"adam\"\n",
        "        activation = \"relu\"\n",
        "   \n",
        "        conv1     = Conv2D(128, (5,5), activation = activation,padding=padding, name = \"conv1\")\n",
        "        max_pool1 = MaxPool2D((2,2))\n",
        "        conv2     = Conv2D(64,  (5,5), activation = activation,padding=padding)\n",
        "        max_pool2 = MaxPool2D((2,2))\n",
        "        conv3     = Conv2D(32,  (5,5), activation = activation,padding=padding)\n",
        "\n",
        "        tied_conv1 = Conv2DTranspose_tied(64,  (5,5), padding=padding, activation = activation, tied_to = conv3, strides=(2,2))\n",
        "        tied_conv2 = Conv2DTranspose_tied(128, (5,5), padding=padding, activation = activation, tied_to = conv2, strides=(2,2))\n",
        "        tied_conv3 = Conv2DTranspose_tied(3,   (5,5), padding=padding, activation = activation, tied_to = conv1)\n",
        "\n",
        "        encoder = Sequential([\n",
        "            conv1,\n",
        "            max_pool1,\n",
        "            conv2,\n",
        "            max_pool2,\n",
        "            conv3,\n",
        "        ])\n",
        "\n",
        "        autoencoder = Sequential([\n",
        "            InputLayer(input_shape=(32,32,3)),\n",
        "            GaussianNoise(0.2),\n",
        "            encoder,\n",
        "            tied_conv1,\n",
        "            tied_conv2,\n",
        "            tied_conv3])\n",
        "        \n",
        "        autoencoder.compile(optimizer = optimizer, loss=\"mse\")\n",
        "        return autoencoder, encoder\n",
        "    \n",
        "    def train_autoencoder(self, x_train, x_test, epochs, batch_size, verbose = 1):\n",
        "        \n",
        "        train_datagen = ImageDataGenerator(\n",
        "          samplewise_center=True,\n",
        "          samplewise_std_normalization=True)\n",
        "        \n",
        "        test_datagen = ImageDataGenerator(\n",
        "          samplewise_center=True,\n",
        "          samplewise_std_normalization=True)\n",
        "        \n",
        "        train_datagen.fit(x_train)\n",
        "        test_datagen.fit(x_test)\n",
        "\n",
        "        train_iterator = train_datagen.flow(x_train, x_train, batch_size=batch_size)\n",
        "        test_iterator  = test_datagen.flow(x_test, x_test, batch_size=batch_size)\n",
        "\n",
        "        history = self.autoencoder.fit_generator(train_iterator,\n",
        "                            epochs = epochs,\n",
        "                            steps_per_epoch = len(x_train) // batch_size,\n",
        "                            validation_data = test_iterator,\n",
        "                            validation_steps = len(x_test) // batch_size,\n",
        "                            verbose = verbose)\n",
        "\n",
        "        return history\n",
        "    \n",
        "        \n",
        "    def build_classifier(self, pretrained = True):\n",
        "\n",
        "        optimizer = \"adam\"\n",
        "        encoder_copy = keras.models.clone_model(self.encoder)\n",
        "        \n",
        "        if pretrained:\n",
        "            encoder_copy.set_weights(self.encoder.get_weights())\n",
        "\n",
        "        classifier = Sequential([\n",
        "               InputLayer(input_shape = self.input_shape),\n",
        "               GaussianNoise(0.2),\n",
        "               encoder_copy,\n",
        "               Flatten(),\n",
        "               Dropout(0.2),\n",
        "               Dense(256, activation = \"relu\"),\n",
        "               Dropout(0.2),\n",
        "               Dense(10, activation = \"softmax\")\n",
        "               ])\n",
        "        \n",
        "        classifier.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "        return classifier\n",
        "    \n",
        "        \n",
        "    def train_classifier(self, data, val_data, epochs, batch_size, pretrained = True, class_weight = None, data_augmentation = False, verbose = 1):\n",
        "        if pretrained:\n",
        "            self.pretrained_classifier = self.build_classifier(pretrained)\n",
        "            model = self.pretrained_classifier\n",
        "        else:    \n",
        "            self.rand_init_classifier = self.build_classifier(pretrained)\n",
        "            model = self.rand_init_classifier\n",
        "        \n",
        "        x_train, y_train = data\n",
        "        x_val,  y_val  = val_data\n",
        "\n",
        "        y_train_ohe = self.one_hot_encode(y_train)\n",
        "        y_val_ohe  = self.one_hot_encode(y_val)\n",
        "\n",
        "        if class_weight:\n",
        "          weights = compute_class_weight(\"balanced\",np.unique(y_train),y_train.reshape(-1,))\n",
        "          class_weight = {class_ind:weight for class_ind, weight in enumerate(weights)}\n",
        "\n",
        "        if data_augmentation:\n",
        "          train_datagen = ImageDataGenerator(\n",
        "          samplewise_center=True,\n",
        "          samplewise_std_normalization=True,\n",
        "          rotation_range = 15,\n",
        "          width_shift_range=0.1,\n",
        "          height_shift_range=0.1,\n",
        "          horizontal_flip=True)\n",
        "\n",
        "        else:\n",
        "          train_datagen = ImageDataGenerator(\n",
        "          samplewise_center=True,\n",
        "          samplewise_std_normalization=True)\n",
        "          \n",
        "        val_datagen = ImageDataGenerator(\n",
        "            samplewise_center=True,\n",
        "            samplewise_std_normalization=True)\n",
        "      \n",
        "        train_datagen.fit(x_train)\n",
        "        val_datagen.fit(x_val)\n",
        "            \n",
        "        train_iterator = train_datagen.flow(x_train, y_train_ohe, batch_size=batch_size)\n",
        "        val_iterator = val_datagen.flow(x_val, y_val_ohe, batch_size=batch_size)\n",
        "\n",
        "        history = model.fit_generator(train_iterator,\n",
        "                            epochs = epochs,\n",
        "                            steps_per_epoch = len(x_train) // batch_size,\n",
        "                            validation_data = val_iterator,\n",
        "                            validation_steps = len(x_val) // batch_size,\n",
        "                            class_weight = class_weight,\n",
        "                            verbose = verbose)\n",
        "        return history\n",
        "\n",
        "\n",
        "    def evaluate_classifier(self, x_test, y_test, key = \"pretrained_classifier\"):\n",
        "        keys = {\"pretrained_classifier\": self.pretrained_classifier,\n",
        "                \"rand_init_classifier\": self.rand_init_classifier}\n",
        "\n",
        "        model = keys[key]\n",
        "        \n",
        "        test_datagen = ImageDataGenerator(\n",
        "        samplewise_center=True,\n",
        "        samplewise_std_normalization=True)\n",
        "\n",
        "        test_datagen.fit(x_test)\n",
        "        y_test_ohe = self.one_hot_encode(y_test)\n",
        "\n",
        "        test_iterator = test_datagen.flow(x_test, y_test_ohe, batch_size=64)\n",
        "\n",
        "        evaluation = model.evaluate(test_iterator)\n",
        "        print(\"Accuracy of \" + key + f\" on test data is {100*evaluation[1]:.2f}%\")\n",
        "        return evaluation\n",
        "\n",
        "\n",
        "    \"\"\" ----- Vizualisation methods and helper functions ----- \"\"\"\n",
        "\n",
        "\n",
        "    def one_hot_encode(self, y_labels):\n",
        "        y_labels_ohe = [np.eye(10,1,k=-int(label[0])) for label in y_labels]\n",
        "        y_labels_ohe = np.array(y_labels_ohe).reshape(-1,10)\n",
        "        return y_labels_ohe\n",
        "\n",
        "    # For this to method to work as intended, the sample-wise standardization has to be turned off\n",
        "    def plot_reconstruction(self, images):\n",
        "        fig = plt.figure(figsize = (10,5))\n",
        "        n_imgs = len(images)\n",
        "\n",
        "        for n in range(1,6):\n",
        "          img = images[n].reshape(self.input_shape)\n",
        "          ae_img = self.autoencoder.predict(img.reshape((1,)+self.input_shape))\n",
        "\n",
        "          plt.subplot(2,5,n)\n",
        "          plt.imshow(img)\n",
        "          frame = plt.gca()\n",
        "          frame.axes.xaxis.set_visible(False)\n",
        "          frame.axes.yaxis.set_visible(False)\n",
        "\n",
        "          plt.subplot(2,5,n+5)\n",
        "          plt.imshow(ae_img.reshape(self.input_shape))\n",
        "          frame = plt.gca()\n",
        "          frame.axes.xaxis.set_visible(False)\n",
        "          frame.axes.yaxis.set_visible(False)\n",
        "    \n",
        "        \n",
        "    def plot_filters(self, key=\"autoencoder\"):\n",
        "        keys = {\"autoencoder\":self.autoencoder,\n",
        "                \"pretrained_classifier\":self.pretrained_classifier,\n",
        "                \"rand_init_classifier\":self.rand_init_classifier}\n",
        "        \n",
        "        model = keys[key]\n",
        "        filters = model.get_weights()[0]\n",
        "        \n",
        "        fig = plt.figure(figsize=(10,10))\n",
        "        \n",
        "        for n in range(min(filters.shape[-1],100)):\n",
        "            filt = filters[...,n]\n",
        "            f_min,f_max = filt.min(), filt.max() \n",
        "            filt = (filt-f_min)/(f_max-f_min)\n",
        "            plt.subplot(10,10,n+1)\n",
        "            plt.imshow(filt)\n",
        "            frame = plt.gca()\n",
        "            frame.axes.xaxis.set_visible(False)\n",
        "            frame.axes.yaxis.set_visible(False)\n",
        "            \n",
        "    def plot_feature_map(self, image, key = \"autoencoder\"):\n",
        "        keys = {\"autoencoder\": self.autoencoder,\n",
        "                \"pretrained_classifier\": self.pretrained_classifier,\n",
        "                \"rand_init_classifier\": self.rand_init_classifier}\n",
        "        \n",
        "        model = keys[key].layers[1]\n",
        "        feature_map_model = Model(inputs=model.inputs, outputs=model.get_layer(\"conv1\").output)\n",
        "        feature_map = feature_map_model.predict(image.reshape(1,32,32,3))\n",
        "\n",
        "        fig = plt.figure(figsize=(20,20))\n",
        "        for n in range(min(feature_map.shape[-1],100)):\n",
        "            plt.subplot(10,10,n+1)\n",
        "            features = feature_map[0,...,n]\n",
        "\n",
        "            f_min,f_max = features.min(), features.max() \n",
        "            features = (features-f_min)/(f_max-f_min)\n",
        "\n",
        "            plt.imshow(features,cmap =\"gray\")\n",
        "            frame = plt.gca()\n",
        "            frame.axes.xaxis.set_visible(False)\n",
        "            frame.axes.yaxis.set_visible(False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1x4h5ymxPfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"------------------Data processing functions------------------\"\"\"\n",
        "\n",
        "def remove_samples(x_data, y_labels, num_samples, categories = (2,4,9)):\n",
        "    \"\"\" \n",
        "    Function to remove a set amount of samples of the given categories.\n",
        "    Returns a new downsampled dataset.\n",
        "    \"\"\"\n",
        "    assert ((y_labels.shape[1] == 1) or (len(y_labels.shape) == 1)) , \"Label shape unrecognized\"\n",
        "    assert (len(x_data) == len(y_labels)), \"Number of datasamples does not match number of labels\"\n",
        "\n",
        "    to_remove = []\n",
        "    for category in categories:\n",
        "      category_inds = np.where(y_labels == category)[0][:num_samples]\n",
        "      to_remove.append(category_inds)\n",
        "\n",
        "    to_remove = np.hstack(to_remove)\n",
        "\n",
        "    x_data = np.delete(x_data, to_remove, axis = 0)\n",
        "    y_labels = np.delete(y_labels, to_remove, axis = 0)\n",
        "\n",
        "    return x_data, y_labels\n",
        "\n",
        "\n",
        "def horizontal_flip(x_data, y_labels, categories = (2,4,9)):\n",
        "    \"\"\" \n",
        "    Function to create horizontal copies of all the images of a given category.\n",
        "    Returns a new dataset upsampled with flipped copies.\n",
        "    \"\"\"\n",
        "    new_imgs = []\n",
        "    new_labels = []\n",
        "    for category in categories:\n",
        "        category_inds = np.where(y_labels == category)[0]\n",
        "        to_flip = x_data[category_inds]\n",
        "        for img in to_flip:\n",
        "            flipped = np.fliplr(img)\n",
        "            new_imgs.append(flipped)\n",
        "            new_labels.append(category)\n",
        "            \n",
        "    new_imgs = np.array(new_imgs)\n",
        "    new_labels = np.array(new_labels).reshape((-1,1))\n",
        "\n",
        "    data   = np.vstack([x_data,   new_imgs])\n",
        "    labels = np.vstack([y_labels, new_labels])\n",
        "\n",
        "    idx = np.random.permutation(data.shape[0])\n",
        "\n",
        "    return data[idx], labels[idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ38sjtArX50",
        "colab_type": "text"
      },
      "source": [
        "# Example usage of the majority downsampling method\n",
        "To use this class for the other methods, the following changes has to be made:\n",
        "\n",
        "1. Minority upsampling: use min_categories when calling remove_samples on the training data, and uncomment the line where horizontal flip is called. Also remember to set horizontal_flip = False for the data augmentation in the train_classifier class method\n",
        "\n",
        "2. Class weights: use min_categories when calling remove_samples on the training data, and set class_weight = True when training the classifiers\n",
        "\n",
        "In the following code block we import the cifar-10 dataset, remove half the samples of all categories, and split the training set into training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDucCoGgdcm2",
        "colab_type": "code",
        "outputId": "b3785ac9-8ede-49a7-a8c8-e2d160e18b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\"\"\" Importing data and separating it into train, test and validation \"\"\"\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "min_categories = (2,4,9)\n",
        "all_categories = np.arange(10)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train/255, x_test/255\n",
        "\n",
        "x_train, y_train = remove_samples(x_train, y_train, 2500, categories = all_categories)\n",
        "x_test,  y_test  = remove_samples(x_test,  y_test,  500 , categories = all_categories)\n",
        "\n",
        "#x_train, y_train = horizontal_flip(x_train,y_train, categories= min_categories) # Uncomment to create horizontally flipped copies\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train, shuffle = True, test_size = 0.05)\n",
        "\n",
        "\n",
        "print(f\"Shape of x-train is {x_train.shape} and y-train is {y_train.shape} \")\n",
        "print(f\"Shape of x-val is {x_val.shape} and y-val is {y_val.shape} \")\n",
        "print(f\"Shape of x-test is {x_test.shape} and y-test is {y_test.shape} \")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x-train is (23750, 32, 32, 3) and y-train is (23750, 1) \n",
            "Shape of x-val is (1250, 32, 32, 3) and y-val is (1250, 1) \n",
            "Shape of x-test is (5000, 32, 32, 3) and y-test is (5000, 1) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfN-ACNr2Rf",
        "colab_type": "text"
      },
      "source": [
        "We then create an instance of the convolutional autoencoder pretraining class, and print the summary of the encoder and autoencoder network. Since we employ tied weights on the transpose convolutions, the number of parameters shown for those layers are only the biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8iJczOZdliq",
        "colab_type": "code",
        "outputId": "305991b2-be8c-455a-eeb6-c7683859948e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "input_shape = (32,32,3)\n",
        "model = CAE_classifier(input_shape)\n",
        "model.encoder.summary()\n",
        "model.autoencoder.summary()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1 (Conv2D)               (None, 32, 32, 128)       9728      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 64)        204864    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 32)          51232     \n",
            "=================================================================\n",
            "Total params: 265,824\n",
            "Trainable params: 265,824\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gaussian_noise_4 (GaussianNo (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "sequential_6 (Sequential)    (None, 8, 8, 32)          265824    \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_tied_7 (Con (None, 16, 16, 64)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_tied_8 (Con (None, 32, 32, 128)       128       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_tied_9 (Con (None, 32, 32, 3)         3         \n",
            "=================================================================\n",
            "Total params: 266,019\n",
            "Trainable params: 266,019\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kqwIdXXsGrF",
        "colab_type": "text"
      },
      "source": [
        "Next we train the autoencoder using a batch size of 64 for 20 epochs. Both the training and testing images are sample-wise standardized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1233RdWjcox",
        "colab_type": "code",
        "outputId": "1a19e3f9-dbd5-4bb6-d41a-bf8a1184f190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "model.train_autoencoder(x_train, x_test, epochs = 20, batch_size = 64)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "371/371 [==============================] - 8s 21ms/step - loss: 0.0382 - val_loss: 0.0178\n",
            "Epoch 2/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0166 - val_loss: 0.0142\n",
            "Epoch 3/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0145 - val_loss: 0.0131\n",
            "Epoch 4/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0133 - val_loss: 0.0122\n",
            "Epoch 5/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0124 - val_loss: 0.0129\n",
            "Epoch 6/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0115 - val_loss: 0.0126\n",
            "Epoch 7/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0112 - val_loss: 0.0119\n",
            "Epoch 8/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0109 - val_loss: 0.0103\n",
            "Epoch 9/20\n",
            "371/371 [==============================] - 7s 19ms/step - loss: 0.0102 - val_loss: 0.0114\n",
            "Epoch 10/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0099 - val_loss: 0.0098\n",
            "Epoch 11/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0099 - val_loss: 0.0097\n",
            "Epoch 12/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0097 - val_loss: 0.0095\n",
            "Epoch 13/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0093 - val_loss: 0.0093\n",
            "Epoch 14/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0092 - val_loss: 0.0092\n",
            "Epoch 15/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0088 - val_loss: 0.0090\n",
            "Epoch 16/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0087 - val_loss: 0.0101\n",
            "Epoch 17/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0086 - val_loss: 0.0091\n",
            "Epoch 18/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0084 - val_loss: 0.0083\n",
            "Epoch 19/20\n",
            "371/371 [==============================] - 7s 20ms/step - loss: 0.0083 - val_loss: 0.0096\n",
            "Epoch 20/20\n",
            "371/371 [==============================] - 7s 19ms/step - loss: 0.0082 - val_loss: 0.0083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb9bc447be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqbKKIlzsTFa",
        "colab_type": "text"
      },
      "source": [
        "Here we plot the 100 first filters obtained from the first convolutional layer of the autoencoder, i.e. the filters applied to the input images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7yVZrgCmLSe",
        "colab_type": "code",
        "outputId": "0c280645-c898-45fa-dc86-3d013e2d85f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "model.plot_filters(\"autoencoder\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAIxCAYAAABNZLJ5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 100 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jaks7OYsgLw",
        "colab_type": "text"
      },
      "source": [
        "Next we train a classifier using the pretrained weights from the autoencoder. The batch size is 128 and we train for 20 epochs with data augmentation (horizontal flip, 15% angle rotation and 10% vertical and horizontal shifts). We then evaluate the converged model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hNJxN4P46Ng",
        "colab_type": "code",
        "outputId": "dc9ba0b5-2127-493c-bdcc-c3a18730cf11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "history = model.train_classifier((x_train,y_train),(x_val,y_val), pretrained = True, batch_size = 128, epochs = 20, class_weight=None, data_augmentation=True)\n",
        "test_results = model.evaluate_classifier(x_test,y_test,key=\"pretrained_classifier\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "185/185 [==============================] - 13s 71ms/step - loss: 1.7281 - acc: 0.3721 - val_loss: 1.4255 - val_acc: 0.4731\n",
            "Epoch 2/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 1.4362 - acc: 0.4835 - val_loss: 1.2126 - val_acc: 0.5766\n",
            "Epoch 3/20\n",
            "185/185 [==============================] - 13s 70ms/step - loss: 1.2945 - acc: 0.5399 - val_loss: 1.1672 - val_acc: 0.5838\n",
            "Epoch 4/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 1.2065 - acc: 0.5748 - val_loss: 1.0104 - val_acc: 0.6462\n",
            "Epoch 5/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 1.1295 - acc: 0.6031 - val_loss: 1.0025 - val_acc: 0.6515\n",
            "Epoch 6/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 1.0814 - acc: 0.6229 - val_loss: 0.9497 - val_acc: 0.6783\n",
            "Epoch 7/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 1.0404 - acc: 0.6365 - val_loss: 0.8947 - val_acc: 0.6898\n",
            "Epoch 8/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 0.9811 - acc: 0.6605 - val_loss: 0.9029 - val_acc: 0.6934\n",
            "Epoch 9/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 0.9531 - acc: 0.6646 - val_loss: 0.8917 - val_acc: 0.7023\n",
            "Epoch 10/20\n",
            "185/185 [==============================] - 12s 67ms/step - loss: 0.9270 - acc: 0.6730 - val_loss: 0.8852 - val_acc: 0.7228\n",
            "Epoch 11/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 0.9062 - acc: 0.6827 - val_loss: 0.8663 - val_acc: 0.7101\n",
            "Epoch 12/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 0.8978 - acc: 0.6910 - val_loss: 0.7902 - val_acc: 0.7353\n",
            "Epoch 13/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 0.8683 - acc: 0.6946 - val_loss: 0.7877 - val_acc: 0.7317\n",
            "Epoch 14/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 0.8426 - acc: 0.7037 - val_loss: 0.8070 - val_acc: 0.7237\n",
            "Epoch 15/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 0.8381 - acc: 0.7068 - val_loss: 0.8048 - val_acc: 0.7255\n",
            "Epoch 16/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 0.8191 - acc: 0.7160 - val_loss: 0.7990 - val_acc: 0.7442\n",
            "Epoch 17/20\n",
            "185/185 [==============================] - 12s 67ms/step - loss: 0.8149 - acc: 0.7155 - val_loss: 0.7634 - val_acc: 0.7424\n",
            "Epoch 18/20\n",
            "185/185 [==============================] - 13s 70ms/step - loss: 0.7897 - acc: 0.7222 - val_loss: 0.7263 - val_acc: 0.7522\n",
            "Epoch 19/20\n",
            "185/185 [==============================] - 12s 68ms/step - loss: 0.7839 - acc: 0.7279 - val_loss: 0.7459 - val_acc: 0.7567\n",
            "Epoch 20/20\n",
            "185/185 [==============================] - 12s 68ms/step - loss: 0.7697 - acc: 0.7291 - val_loss: 0.8005 - val_acc: 0.7353\n",
            "79/79 [==============================] - 1s 7ms/step\n",
            "Accuracy of pretrained_classifier on test data is 73.78%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo6b3aGt0JXw",
        "colab_type": "text"
      },
      "source": [
        "Here we calculate the precision, recall and f1-score for the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvLNfme2p9MH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "e353dc52-c9f3-4bcb-f83c-7768556bcec9"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "      samplewise_center=True,\n",
        "      samplewise_std_normalization=True)\n",
        "\n",
        "test_datagen.fit(x_test)\n",
        "test_iterator = test_datagen.flow(x_test, shuffle=False)\n",
        "\n",
        "y_pred = model.pretrained_classifier.predict_generator(test_iterator)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.82      0.77       500\n",
            "           1       0.88      0.83      0.85       500\n",
            "           2       0.74      0.59      0.66       500\n",
            "           3       0.56      0.50      0.53       500\n",
            "           4       0.69      0.67      0.68       500\n",
            "           5       0.76      0.55      0.63       500\n",
            "           6       0.75      0.81      0.78       500\n",
            "           7       0.70      0.85      0.77       500\n",
            "           8       0.82      0.86      0.84       500\n",
            "           9       0.75      0.90      0.82       500\n",
            "\n",
            "    accuracy                           0.74      5000\n",
            "   macro avg       0.74      0.74      0.73      5000\n",
            "weighted avg       0.74      0.74      0.73      5000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UATRNcNPszD7",
        "colab_type": "text"
      },
      "source": [
        "Here we perform the same training as above, but this time we create a new classifier network without copying the weights from the autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15IbMUay5Z8j",
        "colab_type": "code",
        "outputId": "e1eab9ad-c7e6-4e5d-a0ba-3b372b7390fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "history = model.train_classifier((x_train,y_train),(x_val,y_val), pretrained = False, batch_size = 128, epochs = 20, class_weight=None, data_augmentation=True)\n",
        "test_results = model.evaluate_classifier(x_test,y_test, key = \"rand_init_classifier\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "185/185 [==============================] - 14s 75ms/step - loss: 1.7180 - acc: 0.3707 - val_loss: 1.3911 - val_acc: 0.4905\n",
            "Epoch 2/20\n",
            "185/185 [==============================] - 13s 70ms/step - loss: 1.4282 - acc: 0.4873 - val_loss: 1.2343 - val_acc: 0.5606\n",
            "Epoch 3/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 1.2996 - acc: 0.5345 - val_loss: 1.1085 - val_acc: 0.6168\n",
            "Epoch 4/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 1.1992 - acc: 0.5775 - val_loss: 1.0621 - val_acc: 0.6087\n",
            "Epoch 5/20\n",
            "185/185 [==============================] - 13s 70ms/step - loss: 1.1381 - acc: 0.5983 - val_loss: 1.0156 - val_acc: 0.6569\n",
            "Epoch 6/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 1.0953 - acc: 0.6175 - val_loss: 0.9812 - val_acc: 0.6658\n",
            "Epoch 7/20\n",
            "185/185 [==============================] - 13s 70ms/step - loss: 1.0617 - acc: 0.6279 - val_loss: 0.9305 - val_acc: 0.6720\n",
            "Epoch 8/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 1.0129 - acc: 0.6433 - val_loss: 0.9053 - val_acc: 0.6836\n",
            "Epoch 9/20\n",
            "185/185 [==============================] - 12s 68ms/step - loss: 0.9826 - acc: 0.6541 - val_loss: 0.8781 - val_acc: 0.7094\n",
            "Epoch 10/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 0.9482 - acc: 0.6676 - val_loss: 0.9052 - val_acc: 0.6934\n",
            "Epoch 11/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 0.9340 - acc: 0.6711 - val_loss: 0.8565 - val_acc: 0.6927\n",
            "Epoch 12/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 0.9107 - acc: 0.6807 - val_loss: 0.8174 - val_acc: 0.7157\n",
            "Epoch 13/20\n",
            "185/185 [==============================] - 12s 67ms/step - loss: 0.8906 - acc: 0.6887 - val_loss: 0.8285 - val_acc: 0.7210\n",
            "Epoch 14/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 0.8768 - acc: 0.6971 - val_loss: 0.8260 - val_acc: 0.7237\n",
            "Epoch 15/20\n",
            "185/185 [==============================] - 12s 67ms/step - loss: 0.8698 - acc: 0.6958 - val_loss: 0.7208 - val_acc: 0.7504\n",
            "Epoch 16/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 0.8463 - acc: 0.7028 - val_loss: 0.8266 - val_acc: 0.7201\n",
            "Epoch 17/20\n",
            "185/185 [==============================] - 12s 67ms/step - loss: 0.8370 - acc: 0.7033 - val_loss: 0.7431 - val_acc: 0.7424\n",
            "Epoch 18/20\n",
            "185/185 [==============================] - 12s 66ms/step - loss: 0.8243 - acc: 0.7120 - val_loss: 0.7672 - val_acc: 0.7380\n",
            "Epoch 19/20\n",
            "185/185 [==============================] - 13s 69ms/step - loss: 0.8076 - acc: 0.7158 - val_loss: 0.7434 - val_acc: 0.7326\n",
            "Epoch 20/20\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 0.8050 - acc: 0.7175 - val_loss: 0.8126 - val_acc: 0.7406\n",
            "79/79 [==============================] - 1s 7ms/step\n",
            "Accuracy of rand_init_classifier on test data is 72.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsgCn3EMxzFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8564c79b-de17-453a-ad78-107d5a51610a"
      },
      "source": [
        "test_datagen = ImageDataGenerator(\n",
        "      samplewise_center=True,\n",
        "      samplewise_std_normalization=True)\n",
        "\n",
        "test_datagen.fit(x_test)\n",
        "test_iterator = test_datagen.flow(x_test, shuffle=False)\n",
        "\n",
        "y_pred = model.rand_init_classifier.predict_generator(test_iterator)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.81      0.78       500\n",
            "           1       0.86      0.83      0.85       500\n",
            "           2       0.63      0.64      0.63       500\n",
            "           3       0.66      0.37      0.47       500\n",
            "           4       0.70      0.64      0.67       500\n",
            "           5       0.67      0.61      0.64       500\n",
            "           6       0.68      0.88      0.76       500\n",
            "           7       0.68      0.86      0.76       500\n",
            "           8       0.85      0.82      0.83       500\n",
            "           9       0.80      0.83      0.81       500\n",
            "\n",
            "    accuracy                           0.73      5000\n",
            "   macro avg       0.73      0.73      0.72      5000\n",
            "weighted avg       0.73      0.73      0.72      5000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-FevneQ0E2I",
        "colab_type": "text"
      },
      "source": [
        "Finally, in the following code block we repeat the experiment 10 times and store the model weights as well as the evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWcrOHoEyo-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.makedirs(\"model_weights/pretrained_classifier\")\n",
        "os.makedirs(\"model_weights/rand_init_classifier\")\n",
        "os.makedirs(\"model_weights/autencoder\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOt7KQ9KzE_V",
        "colab_type": "code",
        "outputId": "3a662347-d5e5-4199-e551-76f5f81045fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "histories = {\"autoencoder\":[],\n",
        "             \"pretrained_classifier\":[],\n",
        "             \"rand_init_classifier\":[]}\n",
        "\n",
        "results = {\"pretrained_classifier\":[],\n",
        "             \"rand_init_classifier\":[]}\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "\n",
        "for n in range(10):\n",
        "  print(f\"Experiment {n+1}/{10}\")\n",
        "  model = CAE_classifier(input_shape)\n",
        "\n",
        "  history = model.train_autoencoder(x_train,x_val, epochs = epochs, batch_size = 64, verbose = 0)\n",
        "  model.autoencoder.save_weights(\"model_weights/autencoder/weights_\" + str(n))\n",
        "  histories[\"autoencoder\"].append(history)\n",
        "\n",
        "  history = model.train_classifier((x_train,y_train),(x_val,y_val), pretrained = True, data_augmentation=True,batch_size = batch_size, epochs = epochs, verbose = 0)\n",
        "  model.pretrained_classifier.save_weights(\"model_weights/pretrained_classifier/weights_\" + str(n))\n",
        "  result = model.evaluate_classifier(x_test,y_test, key = \"pretrained_classifier\")\n",
        "  histories[\"pretrained_classifier\"].append(history)\n",
        "  results[\"pretrained_classifier\"].append(result)\n",
        "\n",
        "  history = model.train_classifier((x_train,y_train),(x_val,y_val), pretrained = False, data_augmentation=True, batch_size = batch_size, epochs = epochs, verbose = 0)\n",
        "  model.rand_init_classifier.save_weights(\"model_weights/rand_init_classifier/weights_\" + str(n))\n",
        "  result = model.evaluate_classifier(x_test,y_test, key = \"rand_init_classifier\")\n",
        "  histories[\"rand_init_classifier\"].append(history)\n",
        "  results[\"rand_init_classifier\"].append(result)\n",
        "\n",
        "print(results)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment 1/10\n",
            "79/79 [==============================] - 1s 12ms/step\n",
            "Accuracy of pretrained_classifier on test data is 74.12%\n",
            "79/79 [==============================] - 1s 11ms/step\n",
            "Accuracy of rand_init_classifier on test data is 73.70%\n",
            "Experiment 2/10\n",
            "79/79 [==============================] - 1s 11ms/step\n",
            "Accuracy of pretrained_classifier on test data is 72.58%\n",
            "79/79 [==============================] - 1s 11ms/step\n",
            "Accuracy of rand_init_classifier on test data is 74.12%\n",
            "Experiment 3/10\n",
            "79/79 [==============================] - 1s 11ms/step\n",
            "Accuracy of pretrained_classifier on test data is 73.46%\n",
            "79/79 [==============================] - 1s 12ms/step\n",
            "Accuracy of rand_init_classifier on test data is 70.56%\n",
            "Experiment 4/10\n",
            "79/79 [==============================] - 1s 11ms/step\n",
            "Accuracy of pretrained_classifier on test data is 72.54%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}